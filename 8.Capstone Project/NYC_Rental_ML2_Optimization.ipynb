{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The following work is the second part of the machine learning effort. \n",
    "It aims at answering the following questions:\n",
    "#### 1. Does feature scaling make the prediction results better?\n",
    "Short answer is No. For both logistic regression and random forest, a slight Log Loss increase was observed with feature scaling. For logistic regression, feature scaling make the Log Loss from the training and validation data more consistent. Feature scaling has negligible effect on the predicted feature importance from random forest classifier.\n",
    "\n",
    "This result is surprising that in most cases (esp in neural network or PCA) feature scaling improves the performance of the ML algorithms. However, preprocessing in machine learning is somewhat a very black art and itt depends heavily on the method you use and also on the problem domain.\n",
    "\n",
    "\n",
    "#### 2. The distribution of price is highly skewed. Does logarithmic transformation help improve the prediction results?\n",
    "For both logistic regression and random forest, a slight Log Loss increase was observed with price being replaced by Log(price).\n",
    "#### 3. How do you handle multicolinearity in the data?\n",
    "\n",
    "Considering multicollineariy is important in regression analysis because it can mess with your coefficient estimates; small changes in the data used for estimation may cause wild swings in estimated coefficients. Ridge regression (which is a regularization method) is commonly recommended as a tool for dealing with colinearity. Unlike some penalization methods, it does not put any coefficients to 0 and it shrinks larger coeffiicients by more than small ones (in absolute value).\n",
    "\n",
    "Machine Learning techniques focus on predictive accuracy, all we care about is how we can use a set of variables to predict another set. We don't care about the impact these variables have on each other. For example, you can imagine that multicollinearity in random forests won’t have any effect on prediction Log Loss or accuracy. If variables are highly correlated, you can simply choose one or the other during the tree split process.\n",
    "\n",
    "#### 4. Can we use feature selection method (PCA, forward feature selection, backward feature selection) to speed up the prediction while maintain high accuracy?\n",
    "Ideally yes. But this is not implemented in this work. The goal of this project is to achieve the lowest Log Loss and speed is not the top concern. Besides, PCA may filter out some important information and decrease the prediction capability of our ML models.\n",
    "\n",
    "#### 5. The three classes of interest level is highly unbalanced. How do you handle it in machine learning?\n",
    "See this link for answer: https://www.quora.com/In-classification-how-do-you-handle-an-unbalanced-training-set\n",
    "In this specific problem, we use Log Loss instead of classification accuray to evaluate the performance of the ML models.\n",
    "\n",
    "#### 6. How do you prevent overfitting?\n",
    "Regularization and cross validation. Regularization is implemented in scikit learn. I hold back 30% of the training data and use it for validating the performance of the data.\n",
    "\n",
    "I will focus on ***logistic regression and random forest algorithm*** because logistic regression is the simplist classification algorithm and random forest usually performs well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "sub[\"listing_id\"] = pd.read_json('test.json')[\"listing_id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "train_df = pd.read_json('FE6_train.json')\n",
    "test_df = pd.read_json('FE6_test.json')\n",
    "y = train_df['interest_level']  # 0, 1, 2 three classes\n",
    "train_df = train_df.drop(['interest_level'], axis = 1)\n",
    "combined = train_df.append(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize both the train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined = combined.apply(lambda x: (x-x.mean())/x.std(), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the training data into train/validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "validation_size = 0.30\n",
    "seed = 2018\n",
    "x = train_df.values\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(x, y, test_size = validation_size, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GridSearch to find the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n",
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "params = {'penalty':['l1', 'l2'], 'C': [1, 10], 'max_iter': [50, 100]}\n",
    "lr = LogisticRegression(fit_intercept = True, random_state = seed)\n",
    "clf = GridSearchCV(lr, params, scoring = 'neg_log_loss')\n",
    "result = clf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BestL 0.615051 using {'penalty': 'l1', 'max_iter': 50, 'C': 10}\n"
     ]
    }
   ],
   "source": [
    "# output best prediction accuracy and best lr model parameters\n",
    "print(\"BestL %f using %s\" % (- result.best_score_, result.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This logloss is smaller than 0.722362 using {'C': 1, 'penalty': 'l1', 'max_iter': 100} without feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log Loss score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61898647584720334"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l1', max_iter = 50, C = 10, fit_intercept = True, random_state = seed)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_val_pred_prob = clf.predict_proba(X_validation)\n",
    "log_loss(Y_validation, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61821151933141005"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l1', max_iter = 100, C = 1, fit_intercept = True, random_state = seed)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_val_pred_prob = clf.predict_proba(X_validation)\n",
    "log_loss(Y_validation, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Log Loss is slightly higher than 0.6163 using {'C': 1, 'penalty': 'l1', 'max_iter': 100} without feature scaling.\n",
    "However, the Log Loss score for the training and validation dataset is big (0.7223 vs 0.6163) without feature scaling, indicating\n",
    "the prediction results are not as robust as that with feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "y = clf.predict_proba(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels2idx = {'low':0, 'medium':1, 'high': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for label in [\"high\", \"medium\", \"low\"]:\n",
    "    sub[label] = y[:, labels2idx[label]]\n",
    "sub.to_csv(\"FE6_lr_fe06172018.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forests\n",
    "Use the best parameters without feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57390944266910349"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 3, criterion = 'gini', random_state = seed)\n",
    "rf.fit(X_train, Y_train)\n",
    "y_val_pred_prob = rf.predict_proba(X_validation)\n",
    "log_loss(Y_validation, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log Loss is slightly higher than 0.5706 using the same parameters without feature scaling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "redict feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  4.64689279e-03,   6.06844019e-03,   8.28018002e-03,\n",
       "         9.09173575e-03,   6.17246820e-03,   7.50840565e-03,\n",
       "         4.26012229e-02,   3.92392583e-02,   6.11090522e-03,\n",
       "         4.81395338e-03,   6.01270076e-03,   1.73341497e-03,\n",
       "         4.13756761e-03,   7.78960466e-03,   1.82910771e-03,\n",
       "         1.25073820e-02,   5.12182237e-04,   8.46822313e-03,\n",
       "         4.90484183e-03,   1.00870898e-04,   2.07900746e-06,\n",
       "         8.17699480e-02,   7.24784341e-02,   1.15924309e-01,\n",
       "         9.94999122e-02,   4.31935458e-02,   4.33140920e-02,\n",
       "         4.53115074e-02,   3.70134826e-02,   2.82496263e-02,\n",
       "         6.25095246e-02,   4.77823866e-02,   4.54053323e-02,\n",
       "         5.57224922e-02,   3.92939697e-02])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importances = rf.feature_importances_\n",
    "importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature ranking:\n",
      "1. feature 23 (0.115924)\n",
      "2. feature 24 (0.099500)\n",
      "3. feature 21 (0.081770)\n",
      "4. feature 22 (0.072478)\n",
      "5. feature 30 (0.062510)\n",
      "6. feature 33 (0.055722)\n",
      "7. feature 31 (0.047782)\n",
      "8. feature 32 (0.045405)\n",
      "9. feature 27 (0.045312)\n",
      "10. feature 26 (0.043314)\n",
      "11. feature 25 (0.043194)\n",
      "12. feature 6 (0.042601)\n",
      "13. feature 34 (0.039294)\n",
      "14. feature 7 (0.039239)\n",
      "15. feature 28 (0.037013)\n",
      "16. feature 29 (0.028250)\n",
      "17. feature 15 (0.012507)\n",
      "18. feature 3 (0.009092)\n",
      "19. feature 17 (0.008468)\n",
      "20. feature 2 (0.008280)\n",
      "21. feature 13 (0.007790)\n",
      "22. feature 5 (0.007508)\n",
      "23. feature 4 (0.006172)\n",
      "24. feature 8 (0.006111)\n",
      "25. feature 1 (0.006068)\n",
      "26. feature 10 (0.006013)\n",
      "27. feature 18 (0.004905)\n",
      "28. feature 9 (0.004814)\n",
      "29. feature 0 (0.004647)\n",
      "30. feature 12 (0.004138)\n",
      "31. feature 14 (0.001829)\n",
      "32. feature 11 (0.001733)\n",
      "33. feature 16 (0.000512)\n",
      "34. feature 19 (0.000101)\n",
      "35. feature 20 (0.000002)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA6UAAAHpCAYAAAB6JJp4AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAB0I5QCAADQjVAKAABAN0IpAAAA3fz/m13+qI1iojAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x212743355f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indices = np.argsort(importances)[::-1]\n",
    "print(\"Feature ranking:\")\n",
    "\n",
    "for f in range(train_df.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n",
    "\n",
    "# Plot the feature importances of the forest\n",
    "plt.figure(figsize = (16, 8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(train_df.shape[1]), importances[indices], color=\"g\", alpha = 0.5, align=\"center\")\n",
    "plt.xticks(range(train_df.shape[1]), indices)\n",
    "plt.xlim([-1, train_df.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted feature importance is roughly the same as that without feature scaling despite of minor difference in weight values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion: \n",
    "For both logistic regression and random forest, a slight Log Loss increase was observed with feature scaling. For logistic regression, feature scaling make the Log Loss from the training and validation data more consistent. Feature scaling has negligible effect on the predicted feature importance from random forest classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2. Price Transformation\n",
    "price --> log(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df['log_price'] = train_df['price'].apply(np.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = train_df.drop(['price'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train_df.values\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(x, y, test_size = validation_size, random_state = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.61835944562283085"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression(penalty = 'l1', max_iter = 50, C = 10, fit_intercept = True, random_state = seed)\n",
    "clf.fit(X_train, Y_train)\n",
    "y_val_pred_prob = clf.predict_proba(X_validation)\n",
    "log_loss(Y_validation, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Log Loss is slightly higher than 0.6163 using the same parameters but without price transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57389084849634386"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 200, min_samples_split = 2, min_samples_leaf = 3, criterion = 'gini', random_state = seed)\n",
    "rf.fit(X_train, Y_train)\n",
    "y_val_pred_prob = rf.predict_proba(X_validation)\n",
    "log_loss(Y_validation, y_val_pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Log Loss is slightly higher than 0.5706 using the same parameters without price transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion:\n",
    "For both logistic regression and random forest, a slight Log Loss increase was observed with price being replaced by Log(price)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering multicollineariy is important in regression analysis because it can mess with your coefficient estimates; small changes in the data used for estimation may cause wild swings in estimated coefficients. Ridge regression (which is a regularization method) is commonly recommended as a tool for dealing with colinearity.  Unlike some penalization methods, it does not put any coefficients to 0 and it shrinks larger coeffiicients by more than small ones (in absolute value).\n",
    "\n",
    "Machine Learning techniques focus on predictive accuracy, all we care about is how we can use a set of variables to predict another set. We don't care about the impact these variables have on each other. For example, you can imagine that multicollinearity in random forests won’t have any effect on prediction Log Loss or accuracy. If variables are highly correlated, you can simply choose one or the other during the tree split process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
